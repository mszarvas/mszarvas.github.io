[Ruder: An overview of gradient descent optimization algorithms](https://ruder.io/optimizing-gradient-descent/)
* Batch
* SGD
* Minibatch SGD
* Momentum
* Nesterov
* Adagrad
* Adadelta
* RMSprop
* Adam
* AdaMax
* Nadam
* AMSGrad
* AdamW: Fixes weight decay
* QHAdam: "Averages a standard SGD step with a momentum SGD step"
* AggMo: applies multiple momentum terms
* Parallel / distributed SGD
  * Hogwild
  * Downpour SGD
  * Elastic Averaging SG
* Additional strategies for optimizing SGD
  * Shuffling and Curriculum Learning
  * Batch normalization
  * Early stopping

[Louis Tiao](https://www.linkedin.com/in/ltiao/)[Visualizing and Animating Optimization Algorithms with Matplotlib](Visualizing and Animating Optimization Algorithms with Matplotlib)

Also has: [Save Matplotlib Animations as GIFs](http://louistiao.me/posts/notebooks/save-matplotlib-animations-as-gifs/)

[Ruder: Optimization for Deep Learning Highlights in 2017](https://ruder.io/deep-learning-optimization-2017/)
* Improving Adam
* Decoupling weight decay
* Fixing the exponential moving average
* Tuning the learning rate
* Warm restarts
* SGD with restarts
* Snapshot ensembles
* Adam with restarts
* Learning to optimize
* Understanding generalization

[An updated overview of recent gradient descent algorithms](https://johnchenresearch.github.io/demon/): Does benchmark of optimizers

